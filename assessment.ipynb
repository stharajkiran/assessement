{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "796d7deb",
   "metadata": {},
   "source": [
    "# Task 1: Sentence Transformer Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3fa283",
   "metadata": {},
   "source": [
    "Implement a sentence transformer model using any deep learning framework of your choice. \n",
    "This model should be able to encode input sentences into fixed-length embeddings. Test your \n",
    "implementation with a few sample sentences and showcase the obtained embeddings. \n",
    "Describe any choices you had to make regarding the model architecture outside of the \n",
    "transformer backbone. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71c8abd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kiran\\anaconda3\\envs\\mlops\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertModel, DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421ca5fd",
   "metadata": {},
   "source": [
    "### Implement a sentence transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03a1ca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "transformer_model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e3a5d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample sentences\n",
    "sentence = \"This is the assessement for sentence transformer implementation. Let's embed this sentence and check the length\"\n",
    "# Tokenize and get embeddings\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2db4ffee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 24])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88950cf9",
   "metadata": {},
   "source": [
    "For the given query sentence, the tokenizer generates 24 tokens for the transformer backbone ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c589d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: This is the assessement for sentence transformer implementation. Let's embed this sentence and check the length\n",
      "torch.Size([1, 768])\n",
      "tensor([[[-0.2685, -0.3769,  0.0627,  ..., -0.0342,  0.0350,  0.6689],\n",
      "         [-0.5525, -0.4315, -0.1180,  ..., -0.0320,  0.1906,  0.4065],\n",
      "         [-0.6068, -0.1323,  0.1431,  ...,  0.0917, -0.2161,  0.8538],\n",
      "         ...,\n",
      "         [-0.4668, -0.3988,  0.2348,  ...,  0.0157, -0.3578,  0.3923],\n",
      "         [ 0.3333, -0.3877, -0.0371,  ...,  0.1150, -0.0046,  0.2536],\n",
      "         [ 0.2240, -0.1058,  0.3084,  ...,  0.3252, -0.8726,  0.1843]]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # forward the token ids to the transformer\n",
    "    transformer_output = transformer_model(**inputs)\n",
    "    # We select the first token (CLS token) as sentence embedding \n",
    "    cls_embedding = transformer_output.last_hidden_state[:, 0, :]  # CLS token\n",
    "print(f\"Sentence: {sentence}\")\n",
    "print(cls_embedding.shape)\n",
    "print(transformer_output.last_hidden_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a96f97",
   "metadata": {},
   "source": [
    "This model should be able to encode input sentences into embeddings of length 768."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1861cf",
   "metadata": {},
   "source": [
    "### Testing the implementation with a few sample sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5439240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I like doing assesements.\n",
      "torch.Size([1, 768])\n",
      "Sentence: I love doing NLP works!! Is there more??\n",
      "torch.Size([1, 768])\n",
      "Sentence: I wish to be a AI engineer and make AI products.\n",
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "# Sample sentences\n",
    "sentences = [\"I like doing assesements.\", \n",
    "             \"I love doing NLP works!! Is there more??\", \n",
    "             \"I wish to be a AI engineer and make AI products.\"]\n",
    "\n",
    "# Tokenize and get embeddings\n",
    "for sentence in sentences:\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        # forward the token ids to the transformer\n",
    "        transformer_output = transformer_model(**inputs)\n",
    "        # We select the first token (CLS token) as sentence embedding \n",
    "        cls_embedding = transformer_output.last_hidden_state[:, 0, :]  # CLS token\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(cls_embedding.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5389b9d",
   "metadata": {},
   "source": [
    "### Describe any choices you had to make regarding the model architecture outside of the transformer backbone. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a17a6e5",
   "metadata": {},
   "source": [
    "In the literature, depending on the use case of sentence transformer, we need to design the model architecture. For example, just for sentence classification or sentiment analysis, we could use the  [CLS] token of BERT/DistilledBERT since the [CLS] token captures the overall meaning of the input. If the application of the transformer were to create general sentence embedding, the [CLS] token only will not work since the [CLS] token embedding does not capture the semantic space to understand the similarity.\n",
    "\n",
    "For this work, we only require the sentence embedding to capture the sentence meaning, using [CLS] token should work. For works that require setence similarity, we need to further fintune the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0cef2d",
   "metadata": {},
   "source": [
    "# Task 2: Multi-Task Learning Expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f507dd18",
   "metadata": {},
   "source": [
    "Expand the sentence transformer to handle a multi-task learning setting. \n",
    "1. Task A: Sentence Classification â€“ Classify sentences into predefined classes (you can \n",
    "make these up). \n",
    "2. Task B: [Choose another relevant NLP task such as Named Entity Recognition, \n",
    "Sentiment Analysis, etc.] (you can make the labels up) \n",
    "Describe the changes made to the architecture to support multi-task learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb8e9ae",
   "metadata": {},
   "source": [
    "### Expand the sentence transformer to handle a multi-task learning setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b64b479",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, transformer_model, tokenizer, num_classes, num_sentiments=3):\n",
    "        super().__init__()\n",
    "        self.transformer = transformer_model\n",
    "        self.tokenizer = tokenizer\n",
    "        # Classification and Sentiment heads\n",
    "        self.classification_head = nn.Linear(self.transformer.config.hidden_size, num_classes)\n",
    "        self.sentiment_head = nn.Linear(self.transformer.config.hidden_size, num_sentiments)\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        input_ids, attention_mask = self.tokenizer(sequence)\n",
    "        # Pass through transformer\n",
    "        transformer_output = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_embedding = transformer_output.last_hidden_state[:, 0, :]  # CLS token\n",
    "        # Outputs for each task\n",
    "        classification_output = self.classification_head(cls_embedding)\n",
    "        sentiment_output = self.sentiment_head(cls_embedding)\n",
    "        # return the task scores\n",
    "        return classification_output, sentiment_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16bf66d",
   "metadata": {},
   "source": [
    "### Describe the changes made to the architecture to support multi-task learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d55c84f",
   "metadata": {},
   "source": [
    "For muliti-task learning setting, we chose to train the model to learn a classification task and a sentiment analysis task. For each task, we add a linear layer that feeds on the setence embedding provided by previously designed sentence transformer. The sentence transformer is shared for both linear layers, which makes sense, since the DistillBERT embedding provides good overall summary of the sequence required for both layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0378663",
   "metadata": {},
   "source": [
    "# Task 3: Training Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abf25ff",
   "metadata": {},
   "source": [
    "### Discuss the implications and advantages of each scenario and explain your rationale as to how the model should be trained given the following: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dca8189",
   "metadata": {},
   "source": [
    "1. If the entire network should be frozen. <br>\n",
    "__Implications :__ If we freeze the entire network, then the weights of both the transformer and task heads remain unchanged.  <br>\n",
    "__Advantages :__ This preserves the original backbone model's knowledge and we do not need to train the model. This approach is recommended when we do not have enough data to train the model and when the network is pretrained enough to work reasonably for both tasks.<br>\n",
    "\n",
    "2. If only the transformer backbone should be frozen. <br>\n",
    "__Implications :__ Freezing the transformer model keeps its knowledge intact. So, we only need to train the task-specific heads. <br>\n",
    "__Advantages :__ if the pretrained model are very good, the embedding should be a great representation of the input sequence. Comparitvely, we need much lesser data to train the linear heads than the whole transformer backbone. Hence, we can train the netork with little data to fintune the task specific heads. This approach is recommended if we only need to finetune the task specific head while the backbone model provides a good general represenation fit for the purpose, and when we do no have much data.\n",
    "\n",
    "3. If only one of the task-specific heads (either for Task A or Task B) should be frozen. <br>\n",
    "__Implications :__ Freezing on one task specific head means the weights of that linear layer are frozen while the transformer model and the another linear layer can be trained. <br>\n",
    "__Advantages :__ Let's say the network performs really good on Task A and we want to train the another task-specific head more for Task B. Without freezing the Task A, we cannot make sure that the performance of the Task A reamins same after training Task B again since the weights of both Tasks are being updated. This is related to catastophic forgetting. Hence, if we freeze one head and train the rest of the network, the frozen task head can retain its capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b33473",
   "metadata": {},
   "source": [
    "### Consider a scenario where transfer learning can be beneficial. Explain how you would approach the transfer learning process, including:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8759653",
   "metadata": {},
   "source": [
    "1. The choice of a pre-trained model. <br>\n",
    "The choice of pretrained model depends on the efficiency, performance and compute resources. DistilBERT provides good mix of performance and efficiency for most of the applications of this kind, and has lesser latency and parameters to train. If the main priority is performance, then the larger model is recommended.\n",
    "\n",
    "2. The layers you would freeze/unfreeze. <br> \n",
    "The higher layers are usually finetuned compared to intermediate layers. The lowere layers are kept frozen.\n",
    "\n",
    "3. The rationale behind these choices.  <br>\n",
    "The lower layers are mainly associated to general language rules whereas the higher layers are more specific to final prediction. So, during finetuning, training higher layers retrains to understand new task specific patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a930aa3b",
   "metadata": {},
   "source": [
    "# Task 4: Training Loop Implementation (BONUS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8da2a2",
   "metadata": {},
   "source": [
    "If not already done, code the training loop for the Multi-Task Learning Expansion in Task 2. \n",
    "Explain any assumptions or decisions made paying special attention to how training within a \n",
    "MTL framework operates. Please note you need not actually train the model.  <br>\n",
    "Things to focus on: <br>\n",
    "- Handling of hypothetical data \n",
    "- Forward pass \n",
    "- Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbb8c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion_a, criterion_b, device):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        model (MultiTaskModel): multi task learning model\n",
    "        dataloader (Dataloader): training data loader\n",
    "        optimizer : Optimizer with the specific layers frozen \n",
    "        criterion_a: loss function\n",
    "        criterion_b : loss function\n",
    "        device: cpu or gpu\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        # obtain the data and labels\n",
    "        input_sequence, labels_a, labels_b = batch\n",
    "        input_sequence, labels_a, labels_b = input_sequence.to(device), labels_a.to(device), labels_b.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass where we input the training sentence and obtaining the task scores\n",
    "        classification_output, sentiment_output = model(input_sequence)\n",
    "\n",
    "        # calculate the loss\n",
    "        loss_a = criterion_a(classification_output, labels_a)\n",
    "        loss_b = criterion_b(sentiment_output, labels_b)\n",
    "        loss = loss_a + loss_b\n",
    "\n",
    "         # Calculate accuracy for current batch\n",
    "        predicted_category = classification_output.argmax(dim=1)\n",
    "        predicted_sentiment = sentiment_output.argmax(dim=1)\n",
    "\n",
    "        correct_category_predictions += (predicted_category == labels_a).sum().item()\n",
    "        correct_sentiment_predictions += (predicted_sentiment == labels_b).sum().item()\n",
    "\n",
    "        total_samples += labels_a.size(0)\n",
    "        \n",
    "        # Metrics\n",
    "        classification_accuracy = (correct_category_predictions / total_samples) * 100\n",
    "        sentiment_accuracy = (correct_sentiment_predictions / total_samples) * 100\n",
    "\n",
    "        print(f\"Iteration {iter + 1}, Loss: {loss.item():.4f}, Category Accuracy: {classification_accuracy:.2f}%, Sentiment Accuracy: {sentiment_accuracy:.2f}%\")\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cd8464",
   "metadata": {},
   "source": [
    "Given some hypothetical training data with raw text sequence, label_a and label_b, our model forwards the classification scores and sentiment score. Assuming the optimizer has the learning rate set for layers and the task heads, we calculate the accuracy metrics for each task, and we aim to optimize these metrics while training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
